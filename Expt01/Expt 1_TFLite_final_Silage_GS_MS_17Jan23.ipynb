{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15vaMQDBLomWMa68r4i3sFpjD_bqsaNc-","timestamp":1670414275037}],"toc_visible":true,"mount_file_id":"1KPJ109RCMCgopPluPyelxoInUMvzjRlQ","authorship_tag":"ABX9TyNB+nAwKpvaJAgPuIjO10TO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Background #\n","\n","One of the commonly used educational data sets for Convoluted Neural Networks (CNN)  in image recognition is that of cats and dogs. Although I trained as a veterinary surgeon I am allergic to cats! More pertinent is the size of the databases that are used: the database used in Chollet (2021) is drawn from a Kaggle (https://www.kaggle.com/) database of 2013 that contains 25,000 images of cats and dogs (12,500 images of each) all correctly labelled (https://www.kaggle.com/c/dogs-vs-cats/data). The 'small' data set extracted from this by Chollet (2021) is still 5000 images. Such databases take a lot of time and effort to develop and to correctly label. Such resources are rarely available in the agricultural sector and developing such large databases will be a major barrier to utilisation of CNN technology. \n","\n","Of greater interest to me in the agricultural contect is the TensorFlow Lite example presented by Matt Butler of weevils and earwigs with 200 images of each class (Github: onthesofa/weevil-watch). This size of database is more realistic in an agricultural setting. The objective of this project was to determine if similarly small databases could be sucessfully applied to other image recognition tasks in the agricultural sector. \n","\n","A second objective was to experience all stages of developing a CNN starting with the image collection and labelling and working through to the fine tuning of a suitable CNN. Image collection can be very time consuming but had to happen over a 4-5 week period so was a major constraint. Two databases were considered. The first (as decribed below) was a collection of images on common feeds fed to cattle. A second (backup) database was also collected of images of cattle breeds scrapped from commercial cattle breeding company websites.    \n","\n"],"metadata":{"id":"wNiBtaGrx7gJ"}},{"cell_type":"markdown","source":["# Methods # \n","\n","## Image collection ##\n","\n","Digital photograph images were collected from a total of seven farms and for four feed types \n","1.   Grass silage (GS)\n","2.   Maize silage (MS)\n","3.   Straw\n","4.   Total mixed ration (TMR). A blend of the above three feeds plus other ingredients that is mixed thoroughly and fed to the milking cows. \n","\n","At each farm each feed was photographed approx 40 times. The images were not identical as they were taken at 300 mm intervals across at 10 - 20m wide 'face' but neither are they truely independent. This might weaken the power of the models but the differences between images are likely to be similar to augemented images created within the CNN. \n","\n","### Image size ### \n","Initial estimates for image numbers suggested there would be 7 farms x 4 feeds x 40 repeats = 1120 in total. A modern digital camera/smart phone collects images at 3 to 10 MByte size such that the total database would exceed the project filesize limitations. In addition an early step in CNNs is to reduce the image down to a few hundred pixels in each dimension which would lose most of the fine resolution captured within a large image file. Images were therefore collected on an elderly digital camera (Casio Exlim) with the image size set to VGA Economy which is 640 x 480 pixels. This produced a jpeg file size of around 100 KBytes. Images were taken in varying lighting with or without flash as necessary so there is a range of colour palettes etc. in the images. \n","\n","A series of six experiments were conducted as follows. To avoid wasting time and resources on re-running unnecessary code in Colab each experiment was set up as a seperate colab file: \n","\n","1.   Expt01. A proof of concept model run using a simple TensorFlow Lite model and 303 images of GS and MS from the first four farms visited in early November. If it had not been possible to fit a decent CNN model to this data set the project would have switched to the back up data set of cattle breeds. However a good model was derived and the cattle database was not used. \n","2.   Expt02. Two classes of feeds from five farms and with 213 images in the training set using a full TensorFlow CNN. Images were statified between the training, test and validation datasets based on file size as this could be automated but this probably was not ideal. \n","3.   Expt02b. Data collected for two feedstuff classes from 7 farms were used in this model with data stratification between the three databases based on file name which gave a more random split. 291 images in the training set.  \n","4.   Expt03. Data from 7 farms and with three feed classes with the addition of straw. On farm experience suggested this should be an easy classification problem as the three classes are very different visually. 439 images in the training set.   \n","5.   Expt04. Data from 7 farms and with four feed classes with the addition of TMRs. On farm experience suggested this last class would not be easily  classified as many TMRs look like grass silage. 578 images in the training set, 964 in total.\n","6.   Expt05. Expt 02-04 were run using TensorFlow and this placed limitations of their deployment platforms. TensorFlow Lite has been created to allow the easy generation of a small CNN that can be deployed on microprocessors such as the Raspberry Pi. A simple, low cost deployment could be developed with a simple digital camera to collect the images, a small LCD display to show the results locally and an WiFi/Internet connection to collect the results. Such a device would allow rapid deployment within the agricultural sector. \n","\n","### Convolution Neural Networks used ###\n","\n","Experiments 1 and 5 used the TensorFlow Lite model (https://www.tensorflow.org/lite). Expt01 used a binary classification and Expt 5 used a four-class classification. The models followed the templates developed by Matt Butler at HAU. \n","\n","Experiments 2-4 used the full TensorFlow model (https://www.tensorflow.org/) and used the templates developed and lain out in Chapter 8 of Chollet (2021). Five model iterations were considered and compared to a naive assumption of predictive ability: \n","\n","1.   Initial fully defined CNN \n","2.   Addition of data augmentation \n","3.   Feature extraction with a pretrained model (VGG16)\n","4.   Pretrained base and data augmentation \n","5.   Fine tuned CNN \n","\n","## References ## \n","\n","Chollet F. (2021) Deep Learning with Python 2nd Ed. pp 478. Pubs Manning, NY, USA  \n","\n","\n","\n","\n","\n"],"metadata":{"id":"LXsBkzNux1Y0"}},{"cell_type":"markdown","source":["First pass at creating a CNN - will use Matt Butlers material as a base template.\n","\n"],"metadata":{"id":"ztyU0amSFsvY"}},{"cell_type":"code","source":["# import tensorFlow Lite \n","!pip install -q tflite-model-maker"],"metadata":{"id":"dGiDfDM8J6Ig","executionInfo":{"status":"ok","timestamp":1670589095811,"user_tz":0,"elapsed":102139,"user":{"displayName":"Tom Chamberlain","userId":"18207368645752322846"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"beea418a-6771-4254-c193-ff053b2e7953"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 577 kB 14.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 50.0 MB/s \n","\u001b[K     |████████████████████████████████| 128 kB 47.6 MB/s \n","\u001b[K     |████████████████████████████████| 3.4 MB 10.2 MB/s \n","\u001b[K     |████████████████████████████████| 60.8 MB 112 kB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 45.7 MB/s \n","\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s \n","\u001b[K     |████████████████████████████████| 87 kB 1.4 MB/s \n","\u001b[K     |████████████████████████████████| 10.9 MB 43.2 MB/s \n","\u001b[K     |████████████████████████████████| 840 kB 52.2 MB/s \n","\u001b[K     |████████████████████████████████| 238 kB 55.1 MB/s \n","\u001b[K     |████████████████████████████████| 25.3 MB 2.5 MB/s \n","\u001b[K     |████████████████████████████████| 498.0 MB 12 kB/s \n","\u001b[K     |████████████████████████████████| 352 kB 64.2 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 44.8 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 72.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 51.9 MB/s \n","\u001b[K     |████████████████████████████████| 40 kB 6.8 MB/s \n","\u001b[K     |████████████████████████████████| 216 kB 68.5 MB/s \n","\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["Set up libraries etc"],"metadata":{"id":"h3KYtjwwK2or"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1kvTxmoFr82"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","\n","from tflite_model_maker import model_spec\n","from tflite_model_maker import image_classifier\n","from tflite_model_maker.config import ExportFormat\n","from tflite_model_maker.config import QuantizationConfig\n","from tflite_model_maker.image_classifier import DataLoader\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["Look to sort out repeatability by setting the seeds to a given number "],"metadata":{"id":"qOcCdmSLMUdz"}},{"cell_type":"code","source":["from numpy.random import seed\n","seed(1)"],"metadata":{"id":"Rgd_tP1aGb3F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now want to get data access sorted - this is on my Google Drive but in a sub-sub-directory. Mount my Google Drive "],"metadata":{"id":"DOWT_PXpMy0Q"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JpQ6WyDM6Ud","executionInfo":{"status":"ok","timestamp":1670589351138,"user_tz":0,"elapsed":246245,"user":{"displayName":"Tom Chamberlain","userId":"18207368645752322846"}},"outputId":"9ef74d2b-fcfa-4206-ee7c-3b8066002d44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Now define the folder on My Drive - right from the root. \n","This is the only line needed to be changed to switch between the two 2-class datasets. Ignore the Cattle set for now.  "],"metadata":{"id":"tctv0OvdNYkZ"}},{"cell_type":"code","source":["# image_path = '/content/drive/MyDrive/Assessment_7082_Cattle/Expt01'\n","image_path = '/content/drive/MyDrive/Assessment_7082_Silage/Expt01'"],"metadata":{"id":"5XhmkQ1VNlmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print (image_path)\n","cwd = os.getcwd()\n","print (cwd)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkpR2O9aOtCR","executionInfo":{"status":"ok","timestamp":1670589351138,"user_tz":0,"elapsed":7,"user":{"displayName":"Tom Chamberlain","userId":"18207368645752322846"}},"outputId":"0fcee8fc-10a4-4c42-894c-39f96ff6687e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Assessment_7082_Silage/Expt01\n","/content\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdV8pJKNOx2I","executionInfo":{"status":"ok","timestamp":1670589351453,"user_tz":0,"elapsed":320,"user":{"displayName":"Tom Chamberlain","userId":"18207368645752322846"}},"outputId":"5cd2a989-aa8e-4bab-fa26-fc864e14fdc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"markdown","source":["Load the data and split 80:20. This does not shuffle them.  There are 165 MS and 144 GS images so a 20% split will give 33 and 29 images in the test set. "],"metadata":{"id":"q-qPIdZGWv-n"}},{"cell_type":"code","source":["data = DataLoader.from_folder(image_path)\n","train_data, test_data = data.split(0.8)"],"metadata":{"id":"4vCBcR8tO-oN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create tensor flow model"],"metadata":{"id":"rpmOS-KXXDuF"}},{"cell_type":"code","source":["# model = image_classifier.create(train_data)\n","model = image_classifier.create(train_data, epochs=20)\n","# model = image_classifier.create(train_data, validation_data=validation_data, epochs=10) - would need a validation set "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J55JHh42XCqu","executionInfo":{"status":"ok","timestamp":1670589694260,"user_tz":0,"elapsed":341441,"user":{"displayName":"Tom Chamberlain","userId":"18207368645752322846"}},"outputId":"f693ee5a-9a85-4b64-9f50-7f76abe2bdd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hub_keras_layer_v1v2 (HubKe  (None, 1280)             3413024   \n"," rasLayerV1V2)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 1280)              0         \n","                                                                 \n"," dense (Dense)               (None, 2)                 2562      \n","                                                                 \n","=================================================================\n","Total params: 3,415,586\n","Trainable params: 2,562\n","Non-trainable params: 3,413,024\n","_________________________________________________________________\n","None\n","Epoch 1/20\n","7/7 [==============================] - 23s 2s/step - loss: 0.6338 - accuracy: 0.6250\n","Epoch 2/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2950 - accuracy: 0.9732\n","Epoch 3/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2603 - accuracy: 0.9821\n","Epoch 4/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2674 - accuracy: 0.9911\n","Epoch 5/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2513 - accuracy: 0.9955\n","Epoch 6/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2506 - accuracy: 0.9911\n","Epoch 7/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2418 - accuracy: 0.9955\n","Epoch 8/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2388 - accuracy: 0.9955\n","Epoch 9/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2337 - accuracy: 0.9955\n","Epoch 10/20\n","7/7 [==============================] - 13s 1s/step - loss: 0.2293 - accuracy: 1.0000\n","Epoch 11/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2314 - accuracy: 0.9955\n","Epoch 12/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2295 - accuracy: 1.0000\n","Epoch 13/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2314 - accuracy: 1.0000\n","Epoch 14/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2255 - accuracy: 1.0000\n","Epoch 15/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2302 - accuracy: 1.0000\n","Epoch 16/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2240 - accuracy: 1.0000\n","Epoch 17/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2218 - accuracy: 1.0000\n","Epoch 18/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2240 - accuracy: 1.0000\n","Epoch 19/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2238 - accuracy: 1.0000\n","Epoch 20/20\n","7/7 [==============================] - 11s 1s/step - loss: 0.2249 - accuracy: 1.0000\n"]}]},{"cell_type":"markdown","source":["Evaluate the model "],"metadata":{"id":"RotR6q86Xti8"}},{"cell_type":"code","source":["loss, accuracy = model.evaluate(test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQk7OE4hXxxE","executionInfo":{"status":"ok","timestamp":1670589705252,"user_tz":0,"elapsed":10996,"user":{"displayName":"Tom Chamberlain","userId":"18207368645752322846"}},"outputId":"9879e21a-ce2a-40cb-b007-e6fd8ba4d830"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 7s 2s/step - loss: 0.2164 - accuracy: 1.0000\n"]}]},{"cell_type":"markdown","source":["Now visualise the results on the test data set  with mis-classified images labelled in red.  "],"metadata":{"id":"c0L-JtFgbLc_"}},{"cell_type":"code","source":["# A helper function that returns 'red'/'black' depending on if its two input\n","# parameter matches or not.\n","def get_label_color(val1, val2):\n","  if val1 == val2:\n","    return 'black'\n","  else:\n","    return 'red'\n","\n","# Then plot 100 test images and their predicted labels.\n","# If a prediction result is different from the label provided label in \"test\"\n","# dataset, we will highlight it in red color.\n","plt.figure(figsize=(30, 30))\n","predicts = model.predict_top_k(test_data)\n","for i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(25)):\n","  ax = plt.subplot(8, 4, i+1)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.grid(False)\n","  plt.imshow(image.numpy(), cmap=plt.cm.gray)\n","\n","  predict_label = predicts[i][0][0]\n","  color = get_label_color(predict_label,\n","                          test_data.index_to_label[label.numpy()])\n","  ax.xaxis.label.set_color(color)\n","  plt.xlabel('Predicted: %s' % predict_label)\n","plt.show()"],"metadata":{"id":"48prZEroa0y7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Results show that all cases in the test set are correctly predicted. No need to refine this model but ought to have seperate validation and test sets in future models. \n","\n","This 'Proof of Concent' experiment suggests that can predict silage class from my images so will proceed with fuller TF models and then return to TF Lite with a final model. "],"metadata":{"id":"zB_qzSSdbvKF"}}]}